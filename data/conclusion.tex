% !Mode:: "TeX:UTF-8"
\chapter{总结与展望}

\section{研究总结}
本文围绕“面向博物馆导览的对话式数字人展示设计”这一主题，针对传统数字导览在学习支持方面存在的单向性、碎片化与表达弱关联问题，提出了 CCEG（Conversation--Context--Embodied Guidance）导览框架，并完成了从理论建构、系统设计、工程实现到用户验证的完整研究路径。研究首先在相关理论基础上明确了博物馆学习场景对导览系统的核心要求，即导览不应止于信息播报，而应在多轮互动中持续支持意义建构。基于此，本文提出 CCEG 三维协同机制，将对话组织、情境理解和具身引导纳入统一设计逻辑。

在系统层面，本文依托 MuseGuide 原型完成了关键链路实现，包括流式语音识别、结构化会话编排、情境状态维护以及状态驱动多模态输出。系统能够在连续会话中维持导览主题，并通过语音与行为协同强化信息重点。用户验证结果表明，CCEG 条件在学习理解、互动参与、情境连续和引导感知等维度均优于线性导览条件，说明本文所提出框架具有明确的可行性和实践价值。

\section{研究局限}
尽管本文完成了较系统的研究与实现，但仍存在若干局限。首先，实验样本规模与受试者结构仍有扩展空间，当前结果更能代表原型阶段趋势而非大规模推广结论。其次，系统知识资源主要围绕示例展区构建，在跨馆迁移和跨学科内容适配方面仍需进一步验证。再次，当前多模态表达以状态切换和语义触发为主，尚未引入口型级实时生成能力，在表现细腻度上仍有提升余地。最后，个性化策略目前主要依据短时会话状态，尚未形成长期学习画像与纵向学习追踪机制。

\section{未来展望}
后续研究可沿三个方向继续推进。第一，在应用维度扩展多馆种场景与多任务人群，验证框架在不同文化叙事环境中的稳健性与可迁移性。第二，在方法维度引入更细粒度学习状态评估模型，使系统能够根据用户理解水平和认知负荷动态调节解释深度与引导节奏。第三，在工程维度进一步升级数字人实时驱动能力与多模态同步机制，提升系统在复杂网络环境下的稳定性与表现一致性。

总体而言，本文的研究说明，面向博物馆学习场景的数字人导览系统只有在“内容组织、情境决策、具身表达”三者协同下，才能从“可用”走向“有效”。CCEG 框架的提出与验证为这一方向提供了可复用的方法基础，也为后续文化传播与公共教育中的智能导览研究提供了参考。

此外，从研究方法角度回看，本文采用“框架提出--原型实现--用户验证”的闭环路径，也验证了设计研究在数字人导览方向的可行性。相比仅进行概念讨论或仅进行功能开发，这一路径能够更完整地揭示设计决策与体验结果之间的关系，具有较好的方法论示范意义。未来研究若能在更大规模场景中延续这一路径，将有望形成面向文化遗产智能导览的系统化知识体系。

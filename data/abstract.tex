% !Mode:: "TeX:UTF-8"

\begin{cabstract}
针对当前博物馆数字导览中“信息传达单向、交互深度不足、学习情境响应弱”的问题，本文围绕“面向博物馆导览的对话式数字人展示设计”展开研究。本文以博物馆学习理论、对话式交互理论与具身认知理论为基础，提出 CCEG（Conversation--Context--Embodied Guidance）设计框架，将对话式讲述、情境感知与具身化多模态表达进行协同整合。

在方法上，本文从导览场景需求出发，构建了面向展品讲解的对话策略、基于上下文的情境识别机制以及语音--文本--动作--表情协同的多模态引导机制，并完成系统原型实现。进一步通过用户测试对系统在学习理解、参与感与引导清晰度等维度进行验证。

研究结果表明：基于 CCEG 框架的数字人导览系统能够有效提升用户在博物馆学习场景中的信息理解效率、互动参与意愿与整体体验评价。本文为文化遗产数字传播中的智能导览设计提供了可复用的方法框架与实践路径。
\end{cabstract}

\begin{eabstract}
This thesis addresses key limitations in current museum digital guidance systems, including one-way information delivery, limited interaction depth, and weak contextual adaptation. Focusing on multimodal digital human presentation design for museum guidance, we propose the CCEG framework (Conversation--Context--Embodied Guidance), which integrates conversational narration, contextual understanding, and embodied multimodal guidance.

Based on museum learning theory, conversational interaction theory, and embodied cognition, we design a dialogue strategy for exhibit explanation, a context-aware state modeling method, and a coordinated multimodal expression mechanism across speech, text, gesture, and facial cues. A prototype system is implemented and evaluated through user studies.

Results show that the CCEG-based system improves comprehension efficiency, interaction engagement, and overall guidance experience in museum learning scenarios. This work provides a reusable design framework and practical reference for intelligent guidance in digital cultural heritage communication.
\end{eabstract}
